Get it started. So phone today. You know it's the metadata, so they will go through the different steps that we need to follow to set up a the metadata for any new tenants. So it's gonna as the content. Yeah, that's big deal. So first time we'll start with the disclaimer then and then it will go through the the initial setup and how to sell the, how to configure the the different. Umm. Ohh, metadata like business metadata and technical metadata. What what are the differences and from where they are coming, how the pipelines are set up in the both on Prem and cloud and at the end I will explain how to consume those metadata. So once we generate the data metadata how to consumer and currently we are consuming it. So the disclaimer part is so the session or the documents that we created so far is only talks about the. Make the business metadata that is configure dot setup in the. I know that the for the other tenants are for example, CSV or international markets. They don't have any data governance tools so they they have generating them manually. So this setup is not meant for them. Because they they have to follow a different uh, procedures to get the metadata into the interface layer. Then as far as the consultation function or the way they're consuming. Those part will be still same for relevant. For example the ingestion part and the GDPR part will be same. But to how to get the metadata into the earlier is a bit different for them. So this setup is not going to explain about it. No to her. Hold. The technical metadata is configured so the for the Belgium, DBE tenant technical metadata we normally refer that as an YAML file. So those metadata as configured in centralized project called Octopus, KBC for the other tenants. Still that's not the case. Umm. The metadata files are low. Spread it across within the somewhere in the project or still manually there. They are just taking it. So this this setup is not for them also and the last. The point is, if you want to refer this. The setup of the documentation, the best references the Belgium DBE tenant that's currently up and running in port on permanent cloud. So to start with. And give you the high level design or the the flow from where the data is originating and then how how it is flowing. So I will so I cover 3. I set up a. How how to configure this metadata as a 3 sections so first session is to get the metadata but the source files and the second part is how to set up the MDR process like the how to run your metadata pipelines to generate the interface layer and the third part is how you can consume those metadata through a consultation functions. So to begin with, for to get the metadata. So as I said, become a two parts of metadata technical metadata and business metadata. So technically metadata is normally talks about. Of the the technical information of every data where it is stored in your in you know Data Lake and the security level. The scheme of the dataset sometime and the data group and all other informations. It's everything is about the technical basically how how to write the data, read the data, something like the business metadata and the other side it talks about. The business part, like GDPR configuration ingestion part the fuel fewer that information, so that that's completely maintained in the data governance tool to compare today and Belgium case it's it's going forward it will be it will be only block so. Then so should, so that's example given in the documentation. There's so you can see the data object identified. It's one of the mandatory attribute that that must be present in every technical metadata. The key layer data group. Also the relative path in which tenant and top level. Is for. This information is splendid the in the. OK, I don't find that's the port first time it's time for. But normally it's it's a tenant. The combination of tenant layer key. Umm, so earlier we used to compute everything during the runtime. The couple of weeks back we we decided to. Right in the in the YAML file itself, because in the future you have different plans to get the additional. I'll put but also as part of the data you didn't. So do we don't want. One screen. Don't. Yep. There's no. So this page explains how you have to configure your technical metadata. People decide to also all it looks like they don't. They couldn't fire because PSN. It's just a reference to the table, no? If they they cannot define design. By themselves, it's not really technically in key, so tenant is Debby and the layer is the flow. And the key is space and then it's a combination of these three things. Second, finding the source data set identified so the source data set identifier is something that. You said included recently, I think last week by the injection team injection team. They're all layer team. Speaker. No problem because of some duplication. So they they had a different name with come from the source file to the different. Look identified the device so they were not able to come here together, so they decided to improve the key. If that's what are consulting us or. Yeah, yeah. Yeah, they, they they started meeting. I think you meeting was so lucky, then they support that. So they they were not able to match they just and process with the data object identified. So they wanted to include so that the new identifier to identify how it was referred in the injection process. It's yes. Mostly in the raw layer. I think the only those fields are getting interested other flights they they don't bother. Just start to see a lot of duplication in here and then they all file. If you would look at the top level specifying yeah. The road 7 to see the minutes of the the little different part in the file is already political. You see the the little piece of the. Even. That is one more namespace. You know, if it's about. They will switch on mainframe. We could have like CBT as a base table that can come also have the CBTPSN for instance. But I've seen DT the anthem which is a daily or monthly extract of of the SM 10. I'm wondering if we would and just a monthly file and available file for instance, how how the Yammer would look like. Umm. If he, if he, if he in just a monthly file or think they frequency will be set this month instead of daily there around other parameters also part of this. Could be just. No. No, it's not allowed. OK, I don't think you have. Yeah, listening now, yes, this is not part of this. I think we have, we have a different view. That's the case. Hey I I don't know how they are managing monthly and the date. I know, for example, that some people have. Two different partition types as they create two separate YAML files or the same file, you create two separate YAML files. Just change the partition type. And in that case, they have to modify the key, so I would assume OK, awesome. And to take a note of that. Question and pick it up later. Anyone explanation session now yes. So the next part is about the business metadata. So as I said, the business metadata is currently for for DBE tenant is it's in the Axon. Umm. Of your your. Meeting uh. Those six files that I listed, glossary datasets, attribute policies. Umm and glossary and policies and also attribute attribute. That's a new file that still it. Maybe you're working on it. We we it's not in production yet. So those are the files that we that we use from the business metadata set and the the one of the one of the attribute the mandatory attribute in dataset is the data Object Identifier identifier. So normally. It's Axon extract and instruction set up by the not by data engineering team. So they will bring those files into the raw raw layer. Then I compare listed out the. Come to that simple. If you want, you can log in and see how how they each data set is maintained there and also I have given up a bit of explanation about. Their business meeting our conference pages also in the in the right bottom given a few example attributes. Uh data object and edifier party. I did so. Those are the other attributes that are used for GDPR process. So I said they they are typically coming from the business metadata side. To the sick. The so now when we have these two steps. For you are. Exam business metadata steady meaning when you set up the with the help of the team. When you set up the Axon extract, you are a business metadata. So already the Rowley and now you have your data link definitions as YAML file for your reference or understanding you can consider the octopus KBC. I said no example, but how, when, where, how, how it looks like. So once you have these two set up, the next step is to get the MDR process and also the technical metadata. So to get the technical metadata, we should implemented a plug in so it's a automatically it will export all the Yammer files that are present in the data lake automatically to the and Nexus. And from there we are pulling out the files from from Nexus into the. Yeah, the backup of 4040 daily feed. So whenever you once you include the plugin in in your project in your data like project it will automatically. And that's everything for you. You need to configure certain parameters like if you do which Nexus Channel you want to upload to your sub file and the other optional parameters. That's only relevant for B data link. So for other tenants just you need to configure or configure one one of the parameter leads to which channel you want to configure to upload your. Once you have that. You your YAML file will be automatically pushed into Nexus Channel whenever you build a master master branch and then they with the help of Infra team, it will automatically come to the. But stable for the process. So so how how do you include the plug in the TV over already list the plug in the metadata export plugin and then some of the parameters that I saw explained. So only the first parameter is mandatory parameter for other for other tenants the next parameters are optional because they don't have the explicit Raleigh like DB so only applicable for the on purpose. They should once you configure this then you will have your child support. Once for the DVD's. It's OK. I think only one division problems. That's not that we are probably each fiber or no, no, no, it's it's only so the normally the data the data link project has only the amount files whenever a new if you add a new ML file or if you add a you modify something then automatically it it will push the changes every day Infra team will take the latest M files and put it in the. Intervention. So so now you now you won't say you have your. Ohh, business metadata on the other side you have the the technical metadata how? How how are you linking so the linking point is the data object identified not me take. No rejoined with this super attributes and then populate the data set. So if you are file, your data set is not configured in. In the business metadata or in the technical metadata, then you will not see them on the final interface or access layer for the consumption, so it will be topped out in between or during the process. And so how how does how does this technically meeting? How does this MDR process works? So in the explanation it said, now you have the file in Nexus and from there with the help of Infra team you are file will be put, put in the batch server. So from there I will. But the visit jobs that normally friends every day to process all the files so it because the. A parameter that you are able to present in the server. So when when when I infra team places that file in the server to read it or I consume that file, those files for the for the generation of the technical metadata business date is like when you want to when you want to process it. That is what the business metadata is is it has two parts. One is their data set and the attribute level metadata. So those those two jobs are depending on the technical metadata. So those two jobs can be set up parallelly but they have dependency with the technical metadata. First we have to generate the technical metadata and then process the business metadata in parallel. I was talking. Yes. Yep, but babies have few incidents. Because uh. To to generate the business metadata we need. We are depending on two things. One is the technical metadata and the other side is Axon extract. So those five files should also be coming to the Raleigh. So they the incidents that you had this morning is because. They Axon extracts were not present and that's the reason the jobs are failing smart. So that's the first part about the on Prem setup for the for the Edison setup is big different because for on Prem we have the exporter plugin that pushes the YAML file automatically into the that server every day. But for cloud we don't have the solution yet because we cannot put the file somewhere from the from the necklace. So. We normally uh, then when we generate, when we create the image it. Talking since octopus came CS or your data lake is data link, project is the dependency of your metadata project. So it normally packages all the files in. Then within your image so when you. When you run or when you run the technical metadata pipeline, you have to refer the part of for the the image itself. So in the in the in the. It's flammatory or mentioned that for the for the Edison platform you have to refer the data link which is present within within your image image itself. So that's the uh, still a manual process with respect to dissent platform. It's not as automatic like on print situation, but we are working on a solution to ingest the file directly so that that that that part can also be automated. But that's not there yet. So now we have covered two parts. 1st The the source part from for both business and technical metadata. Now we have and then we explained about the MTR process, how to set up your pipeline to get it the interface layer. So the end of the second step the all the points that are there out. Now you have the three metadata and the access layer for the interface layer that are ready for the the consumption. Uh, so content consumption is that we we decided that not everybody took want directly read the file so it is added to implement the consultation functions as part of the data link data link project itself did sampling DB case. It's octopus, KBC. It provides set of functions. So whoever wants to consume or receive the metadata, they can use those functions to. Well, to read metadata for using the Data Object identifier as a as a parameter. Currently. Two major consumers. So when you say consumer, it's a processor intuition and and the GDPR. So in the documentation by it's explained how to set up a those consultation functions and what are the mandatory parameters. And we also have a certain enums that needs to be. Configured everything is explained in the documentation. So the last part is. It said GDPR and ingestion or consuming those metadata what what they actually they are consuming and why they are using it. Uh, so intuition. And I'm not that familiar with I can explain about bit about the GDPR process. So, so so the set set of attributes that are coming from the business metadata side example contains GDPR. That will that will tell us if that data set is meant for GDPR process or not. So based on that only the GDPR process will kick in when you read the read you read the data set. That in the the subject types what it is it a party or a group or something like that and they identifier which attribute that you have to use it to join with party privacy and the other interesting attributes like reference attribute that's one we use for reference date is one that we use for. Data retention. So this this attribute will indicate for each data set which attribute should be used or. You can. The proverb considered to apply data retention for dataset. So. For business goal. Well, the, the the business metadata and technical metadata is linked and then at the end the consuming process will benefit out out of it. One more information we, uh, all the consumer functions normally reads the latest partition of metadata because we don't want any process to fail if for some reason if the metadata is not generated for particular day. Yes, it it has heavy dependency with the injection and the GDPR. It's almost like every single day in the same process and the dataset it depend on the metadata. So we always we normally read the. Ohm latest partition of the data set but we also as in in the consultation functions you have an option to read a specific partition of the partition of the data set. If you if you are interested to know the historical layout of the metadata. Here. So. To support it close, I want to give you an video is like how? The the whole floor is the the whole flow is chained together, syntactic if you are. If you are adding a new YAML file in your in your data like project how that is transformed. Here's some has a metadata through June that for example if you are in Group name in including or a file in octopus scales your we are including it or modifying it. Now when you merge it with the participants automatically the the Jenkins will kick in and the. You look like in that I explained will automatically push the. Is it you are all the? Yeah, the Yammer files and then. Put it in the Nexus. From there it's automatic process that runs every day or morning that will download the Yammer file and then place it in the batch server. What is the process that runs daily? Uh, yes. No, the the 5th is the well pipeline. The 4th one is by end 14 so they it's also automatic process. But what I'm Prem is everything is automatic so so you will you once you include the file Jenkins is automatic pybuilder is automatic and they infra processes also automatically that will that will go every day. It's scheduled. Yes, that's OK. That's fine, actually. When you change and. Yes. Yes. But let us cloud. So after I. Once you met your changes, uh, you have to take certain manual steps. The first thing is. You have to trigger a check. It's built off a metadata project. Why we have to deploy the metadata project because as I said in the technical metadata we are referring the. Uh, the part from the image itself, so it is necessary that you have to rebuild the embedded data project to generate your new image. And once you trigger the jenkinsfile then the rest of the steps are automatic, so it will automatically create the Docker image and then you are the image will be deployed in the developed dev environment automatically. Then if you want to promote it to accept, let's see the manual process that you have to take. So once a day. See how scheduled meeting. To have some not. No, I'm not fine. I'm still on the first one. Yeah. First one is daily. Yeah. Yes. But if you if you want to get it immediately, then you have to ask Infra team to push it. And then run your fizzy job. So you will have it immediately. But for the Edison? Yeah, I'm currently. I'm doing it time to time manually. I will trigger the metadata projects built. It creates the image and then deploy it, but however the. The the business metadata is already coming to cloud every day, so I have a the scheduled adapts that produces the business metadata automatically, but only if you want to update in your technical metadata you have to process this. I mean you have to rebuild metadata project. Yeah, it's the low, full explanations. But all this informations are. Opted in the documentation system. But but I have to go to the documentation. And we pretty show me. So I think that would page with the same coordinate and this presentation that another commentation service needed for the other colleagues. But that's the documentation that you just went over them. Yes. So there's no, but yeah, this it's not available. With the same explanation, so bailable as the document so. The the first the section explains about their technical metadata. When you have, when you have your business metadata from. In the data governance too, how to configure your? Uh. That the US jobs and the business metadata and the you're the Edison setup the where you can find the decks and they're implement implementation and the second section we have explanations about the Pybuilder plugin and it's parameters. They work also. What are the different modules that are included in this this the first only the first part we are using for the metadata the the next two two parts we are using it for a different purpose, this one the 2nd, 2nd plugin. We are on the module in the plugin that we're using for. Using to feed the information for the SSP request creation that those two sections are not relevant for this. Ohh the decision. And then the last part is about. Spoke to the the the the consultation functions and where you can find the the details like how you can call. But the the consultation functions and in the tutorial we also have the given a general explanations about. Both the whole data is flowing and the different components and that are involved in it. Please. Just like I had went through. So. So sorry. Yeah, this this one should should I think only this explanation has to go to the right. But then the still be. How to explain about the? Good. But the manual way of generating the metadata that the documentation is not ready. So once we have a a full set of setup of 1 tenant then we know what are the different steps that had involved. Then we will generate the documentation for that. And by the time if we have the automatic ingestion process. People. And protector explanations also in the top. My question about the artifacts that generate artifacts final one which the owner of that. So you mean I wish responsible for it? You mean the data? You will, you will. Though because I. I'm working on this V next and hearing you told me the the location on the street, but it's CSV file to see. Yeah, I thought the so I'm I'm wondering like just contacts and you can contact me. I OK because I have very much like to change it because I don't know what the possibilities are. So you can touch. So the CSV is then for the next yeah for octopus we next so the current all this explanation is. Thought generating the parking fine. But going forward we will generate a unified structure. That's the new design and that was proposed. But so it's in a different structure that's used by. Swimming. OK. But but that's only for the clock. I think the jobs are running in our on premises, but primarily it's meant for problems. Things are to this group because I'm very interested, so like I think with me. You don't smoke comments from my side. I see that the Bible there is was there too. Because the Yammer files. Look up to Nexus. I hope he's will not become a common pattern where we manage. Files to pybuilder as it's meant for. This team and. We present this with some channel situation where it comes about upgrades or. Yeah, I mean something small and block the situation, but maybe in the. On the long run. By building is not met. Yeah, let's here we have packaging. Deeper. Of 18 or T. Wow. Four months 4. You know. Interesting point. I'm interested in the job. Something else? Yep, Yep, maybe to get the Peter yes. Yeah. Yeah, like the somewhat only specified for testing. You might have different people. It starts going. Yeah. So for certain term of governance support. I think out of all the plugins this one is different one so non says the rest of the plugins are really supporting the building and testing but this one really packages the data. You go to drawing again. Something else like to stress? I think it's under to talking all explanation which. This is easy. See this this whole metadata courses including the MBR generates 3 output files on which the count on top of which the consultation functions run. And so I would like to stress the importance that these files are correct. If they're not those consoles, consultation functions will not work, and so basically nobody can use chemical or do anything judge. So I had that happen and acceptance that these. Uh files were written function which will not make that complete datasets. They were the latest available files. And none of the consultations mentions work and all of acceptance with foundation. Everything was not working anymore because they have the consultation functions. Switch was running on top of broken files, so it's a really critical part of the infrastructure. So if something strong that a lot of people are in fact. So also if you have doing maintenance. It's important to know that this can be an effective there are partially written files. Then we have mitigated most of the risk that because if the file is not written, that will just fall back to the latest available one. Partially or than it can partially return yes, can create the mess. What happened? The jobs failed or, yeah, was declared apartment where the I think the jobs were were aborted somewhere halfway the processing and. Yeah. The full group text file secure ordering moved the file switch apart. You're pushing it them off and this case. The afternoon of the first be the problem was resolved that he stopped the jobs and there was fully written afterwards. So. That's fixed the situation, but just to know the abortions, I'm not trying to. Approved the process here or tell you we need to improve the process, but just to stretch them importance a lot of people are the timing on this consultation functions and these files to be there so if something is wrong we should give it another detention. That was never feels like that. These stupid God, it's just that generate those three sets, OHP that generate the screen. Yeah, so we get that that we should fix it. Yeah, we have instant. But if there is an incident like this morning, they whistle accepting united so that there was a production incident about these files and it was one of the missing. But. Dates. Directories. Well, the only thing I can do is try rerunning those those USC jobs and they fail. Or are they succeed or they are failed, but I don't know anything about the size of those files or or middle even know where and where where they are. I think when it generously partition filling the job should fail, no? Oh, but the judiciary position file, for example data submitted data pipeline. If that generates a partial file, then the job would fail. Yes. Yeah. If I spect the the the the USC job like this morning, it's actually we're missing Peter. Yeah, that, that that the run didn't even start. Yeah, that's that's OK file. So I think that the the version of yesterday is still the place. Yeah, that's OK. So because you have the full file of yesterday. So the consultation functions still work, but what Martin referring is. You know something the infrastructure or they pay spark job has some problem. Partially ribbon. You can have. Profile picture of Sivaprakash Shanmugam. Sivaprakash Shanmugam14:45 But we can't see anything related to infrastructure. So we think we can do is uh distort the file size. 14:45 Ricardo Castañeda Rueda left the meeting You have that. Go check it somehow, and yeah, I'll hurt you if file size is not the normal silly expenses, that doesn't happen. Here but. That's just to make a point. These files are important and we should be. One thing we might be able to do. Less it's it's linked. The generation or creation of those Axon files with the execution of the flow. Now we have just providing a time saver and USC every day. We're finding love without even knowing if docs and files and then. But probably he's asking files also have USC workflows which we can link our process. I think the intuition process and I don't know if they have a USA. It's identically will return one, the Raleigh saying just in process. But we can check if upon the file arrival we can pick up this job. So possible. The only thing which you can take to. And clear glass incidents. But there's also one other thing I will make a story for that. You can write the files. We can prove location once everything is written, remove it on Hadoop. That's an atomic move. And then you will never have an issue with partial everything files please. Because it will be there or it won't be there. If you're if you're also stops halfway, well, process has failed, but the file is not written on final location. Yeah, it's also possible it's it's rather simple fix, but. What are join will also help to solve. This likely include reporting. To be forwarded to Long Lake. We should data must have data on this post about metadata and and it's kind of because this was talking about file sizes and think about data quality checks. So and then if files come from action, if you put in the scheduler it will look for those files. But I don't know how far this integration is possible. Because they're using scheduler have functionality to look for a file to start the process what we got. Just looks every. It's what it's exposed on folder. I don't know exactly how. Yeah, it's it's Hadoop, Hadoop file monitor or something. It is there, they don't know how that will work with cloud. But but still there is a process which children in the. 2nd. Open quote.
